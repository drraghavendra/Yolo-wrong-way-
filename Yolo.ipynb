 -*- coding: utf-8 -*-
"""
# Wrong Way Detection Using YOLO (Google Colab)

This Google Colab notebook provides a comprehensive solution for detecting vehicles moving in the wrong direction using the YOLO (You Only Look Once) object detection algorithm and centroid tracking.

**Objective:**
The primary goal is to identify vehicles that are travelling against the designated flow of traffic in a given video stream or set of images. This is achieved by:
1.  Detecting vehicles using a pre-trained YOLO model.
2.  Calculating the centroid of each detected vehicle's bounding box.
3.  Tracking these centroids over successive frames to determine the vehicle's direction of movement.
4.  Applying a custom logic to classify movement as "correct" or "wrong" based on predefined traffic flow patterns (e.g., a detection line or zone).

**Optimization for Better Outputs:**
-   **YOLOv8:** Utilizes the latest YOLOv8 model from `ultralytics` for improved accuracy and speed compared to older versions.
-   **Centroid Tracking:** Simple yet effective method for determining motion direction. For more robust tracking, consider integrating more advanced trackers like DeepSORT.
-   **Configurable Parameters:** Key parameters like confidence thresholds, detection lines, and history buffer are made configurable for easy tuning.
-   **Visual Feedback:** Provides clear visual indicators (bounding boxes, centroids, direction lines, and "WRONG WAY" labels) for easy understanding and debugging.

"""

# ----------------------------------------------------------------------------
# 1. Environment Setup and Library Installation
# ----------------------------------------------------------------------------

# Ensure you are running on a GPU runtime for optimal performance.
# Go to Runtime -> Change runtime type -> Hardware accelerator -> GPU.

# Install necessary libraries. ultralytics provides a convenient API for YOLOv8.
# opencv-python is essential for video processing and drawing.
# tqdm is for progress bars.
print("Installing required libraries...")
!pip install ultralytics==8.0.0 opencv-python tqdm --quiet
print("Libraries installed successfully.")

import cv2
import numpy as np
from ultralytics import YOLO
from collections import deque
import time
from tqdm.notebook import tqdm # For progress bar in Colab

# ----------------------------------------------------------------------------
# 2. YOLO Model Loading
# ----------------------------------------------------------------------------

# Load a pre-trained YOLOv8 model.
# 'yolov8n.pt' is a nano-sized model, good for quick inference and Colab.
# You can choose larger models like 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt'
# for higher accuracy, but they require more computational resources.
print("\nLoading YOLOv8 model...")
model = YOLO('yolov8n.pt')
print("YOLOv8 model loaded.")

# Define the classes we are interested in.
# YOLO models are trained on COCO dataset, where 'car', 'truck', 'bus', 'motorcycle'
# are typically classes 2, 3, 5, 7 respectively.
# Adjust this list based on your specific needs or if you train a custom model.
target_classes = [2, 3, 5, 7] # COCO classes: car, motorcycle, bus, truck
class_names = model.names # Get class names from the model

print(f"Targeting vehicle classes: {[class_names[cls_id] for cls_id in target_classes]}")

# ----------------------------------------------------------------------------
# 3. Direction Detection Logic - Core of Wrong Way Detection
# ----------------------------------------------------------------------------

# Global variables to store detected object information across frames for tracking
# A dictionary to store centroid history for each unique object ID.
# Key: object_id (int), Value: deque of (x, y) centroids
object_centroid_history = {}
# A dictionary to store the last known direction for each object ID.
# Key: object_id (int), Value: 'correct' or 'wrong'
object_direction_status = {}

# Configuration for detection
CONFIDENCE_THRESHOLD = 0.5 # Minimum confidence to consider a detection valid
IOU_THRESHOLD = 0.5        # Intersection over Union threshold for Non-Maximum Suppression (NMS)
HISTORY_BUFFER_SIZE = 10   # Number of previous centroids to store for direction calculation
MIN_MOVEMENT_PIXELS = 10   # Minimum pixel movement required to consider a direction change

# Define a virtual "detection line" or "region" to determine direction.
# This is crucial for defining what constitutes "correct" vs. "wrong" way.
# Example: For a road where traffic flows from left to right, a vehicle
# moving from right to left across a specific vertical line would be "wrong way".

# Define a horizontal line (for vertical traffic flow, e.g., upward is correct)
# Or define a vertical line (for horizontal traffic flow, e.g., rightward is correct)

# For simplicity, let's assume a scenario where:
# - Vehicles are expected to move primarily from TOP to BOTTOM of the frame.
# - A "wrong way" vehicle would be moving from BOTTOM to TOP.
# We'll define a horizontal "detection line" in the middle of the frame.
# Crossing this line from bottom to top implies wrong way.

# --- Customizable Parameters for Direction Logic ---
# You will need to adjust these based on your specific video and traffic flow.
DETECTION_LINE_Y = None # Will be set based on frame height
DETECTION_LINE_X = None # Will be set based on frame width (if using vertical line)

# Type of detection logic: 'vertical_flow' or 'horizontal_flow'
# 'vertical_flow': Assumes traffic moves predominantly vertically.
#                  Requires a horizontal line (DETECTION_LINE_Y).
#                  e.g., correct is top to bottom, wrong is bottom to top.
# 'horizontal_flow': Assumes traffic moves predominantly horizontally.
#                   Requires a vertical line (DETECTION_LINE_X).
#                   e.g., correct is left to right, wrong is right to left.
FLOW_TYPE = 'vertical_flow' # Change to 'horizontal_flow' if your video dictates

# Expected direction for 'vertical_flow': 'down' or 'up'
# Expected direction for 'horizontal_flow': 'right' or 'left'
EXPECTED_DIRECTION = 'down' # E.g., for 'vertical_flow', 'down' means correct traffic flows from top to bottom.

def calculate_centroid(bbox):
    """
    Calculates the centroid (center point) of a bounding box.
    bbox format: [x1, y1, x2, y2]
    """
    x1, y1, x2, y2 = map(int, bbox)
    cx = (x1 + x2) // 2
    cy = (y1 + y2) // 2
    return (cx, cy)

def determine_direction(object_id, current_centroid, frame_width, frame_height):
    """
    Determines the direction of an object based on its centroid history
    and classifies it as 'correct' or 'wrong' way.
    """
    if object_id not in object_centroid_history:
        object_centroid_history[object_id] = deque(maxlen=HISTORY_BUFFER_SIZE)
    object_centroid_history[object_id].append(current_centroid)

    history = object_centroid_history[object_id]

    if len(history) < 2:
        return None # Not enough history to determine direction

    # Compare current centroid with an older one in the history to smooth out jitters
    old_centroid = history[0]
    current_centroid = history[-1]

    dx = current_centroid[0] - old_centroid[0]
    dy = current_centroid[1] - old_centroid[1]

    direction = None
    is_wrong_way = False

    # Set detection line if not already set (based on first frame's dimensions)
    global DETECTION_LINE_Y, DETECTION_LINE_X
    if FLOW_TYPE == 'vertical_flow' and DETECTION_LINE_Y is None:
        DETECTION_LINE_Y = int(frame_height * 0.5) # Middle of the frame
    elif FLOW_TYPE == 'horizontal_flow' and DETECTION_LINE_X is None:
        DETECTION_LINE_X = int(frame_width * 0.5) # Middle of the frame

    if FLOW_TYPE == 'vertical_flow':
        if abs(dy) > MIN_MOVEMENT_PIXELS:
            if dy > 0: # Moving downwards (y increases)
                direction = 'down'
            else: # Moving upwards (y decreases)
                direction = 'up'

            # Wrong way logic for vertical flow
            # If expected is 'down' but vehicle is moving 'up'
            # And it's crossing or has crossed the detection line towards the wrong zone
            if EXPECTED_DIRECTION == 'down' and direction == 'up' and current_centroid[1] < DETECTION_LINE_Y:
                is_wrong_way = True
            # If expected is 'up' but vehicle is moving 'down'
            elif EXPECTED_DIRECTION == 'up' and direction == 'down' and current_centroid[1] > DETECTION_LINE_Y:
                is_wrong_way = True

    elif FLOW_TYPE == 'horizontal_flow':
        if abs(dx) > MIN_MOVEMENT_PIXELS:
            if dx > 0: # Moving rightwards (x increases)
                direction = 'right'
            else: # Moving leftwards (x decreases)
                direction = 'left'

            # Wrong way logic for horizontal flow
            # If expected is 'right' but vehicle is moving 'left'
            # And it's crossing or has crossed the detection line towards the wrong zone
            if EXPECTED_DIRECTION == 'right' and direction == 'left' and current_centroid[0] < DETECTION_LINE_X:
                is_wrong_way = True
            # If expected is 'left' but vehicle is moving 'right'
            elif EXPECTED_DIRECTION == 'left' and direction == 'right' and current_centroid[0] > DETECTION_LINE_X:
                is_wrong_way = True

    return 'wrong' if is_wrong_way else 'correct' if direction else None


# ----------------------------------------------------------------------------
# 4. Inference Function
# ----------------------------------------------------------------------------

def process_video_for_wrong_way(video_path, output_path="output_wrong_way.mp4"):
    """
    Processes a video file, performs YOLO detection, tracks centroids,
    and identifies wrong-way vehicles.
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video file {video_path}")
        return

    # Get video properties
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # Define the codec and create VideoWriter object
    # For Colab, 'mp4v' codec usually works well for MP4 output.
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    print(f"\nProcessing video: {video_path} (Total frames: {total_frames}, FPS: {fps})")
    print(f"Output video will be saved to: {output_path}")

    # Initialize a simple object tracker (e.g., using a dict for object IDs)
    # This is a very basic tracker. For production, consider robust trackers like DeepSORT.
    # We'll use a simple IOU-based assignment for assigning new detections to existing objects.
    next_object_id = 0
    tracked_objects = {} # {object_id: {'bbox': [x1,y1,x2,y2], 'last_seen_frame': int}}

    # Reset global history for each video processing run
    global object_centroid_history, object_direction_status, DETECTION_LINE_Y, DETECTION_LINE_X
    object_centroid_history = {}
    object_direction_status = {}
    DETECTION_LINE_Y = None # Reset line for new video
    DETECTION_LINE_X = None # Reset line for new video

    for frame_idx in tqdm(range(total_frames), desc="Processing Frames"):
        ret, frame = cap.read()
        if not ret:
            break

        # Perform YOLO detection
        # stream=True for generator output, faster for videos/streams
        results = model(frame, conf=CONFIDENCE_THRESHOLD, iou=IOU_THRESHOLD, classes=target_classes, verbose=False)

        current_frame_detections = [] # Store {'bbox': ..., 'conf': ..., 'cls': ...}

        # Process detections and update/assign object IDs
        if results and results[0].boxes:
            for box in results[0].boxes:
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                conf = box.conf[0].cpu().item()
                cls = int(box.cls[0].cpu().item())

                current_frame_detections.append({
                    'bbox': [x1, y1, x2, y2],
                    'conf': conf,
                    'cls': cls,
                    'centroid': calculate_centroid([x1, y1, x2, y2])
                })

        # --- Simple IOU-based Object Association ---
        # This part tries to re-associate detections in the current frame with
        # previously tracked objects based on IOU.
        new_tracked_objects = {}
        matched_current_indices = set()

        for obj_id, obj_data in tracked_objects.items():
            best_match_idx = -1
            max_iou = 0.0

            for i, current_det in enumerate(current_frame_detections):
                if i in matched_current_indices: # Skip already matched detections
                    continue

                bbox1 = obj_data['bbox']
                bbox2 = current_det['bbox']

                # Calculate IOU
                x_overlap_min = max(bbox1[0], bbox2[0])
                y_overlap_min = max(bbox1[1], bbox2[1])
                x_overlap_max = min(bbox1[2], bbox2[2])
                y_overlap_max = min(bbox1[3], bbox2[3])

                overlap_width = max(0, x_overlap_max - x_overlap_min)
                overlap_height = max(0, y_overlap_max - y_overlap_min)
                intersection_area = overlap_width * overlap_height

                area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
                area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
                union_area = area1 + area2 - intersection_area

                iou = intersection_area / union_area if union_area > 0 else 0

                if iou > max_iou:
                    max_iou = iou
                    best_match_idx = i

            if best_match_idx != -1 and max_iou > 0.3: # Threshold for considering a match
                # Update existing tracked object with new detection
                new_tracked_objects[obj_id] = {
                    'bbox': current_frame_detections[best_match_idx]['bbox'],
                    'centroid': current_frame_detections[best_match_idx]['centroid'],
                    'cls': current_frame_detections[best_match_idx]['cls'],
                    'last_seen_frame': frame_idx
                }
                matched_current_indices.add(best_match_idx)
            else:
                # If no good match, keep the old object for a few frames
                # (You might want a more sophisticated disappearance handling)
                if frame_idx - obj_data['last_seen_frame'] < 5: # Keep for 5 frames without detection
                     new_tracked_objects[obj_id] = obj_data


        # Add new detections (those not matched with existing objects)
        for i, current_det in enumerate(current_frame_detections):
            if i not in matched_current_indices:
                new_tracked_objects[next_object_id] = {
                    'bbox': current_det['bbox'],
                    'centroid': current_det['centroid'],
                    'cls': current_det['cls'],
                    'last_seen_frame': frame_idx
                }
                next_object_id += 1
        tracked_objects = new_tracked_objects


        # --- Process Tracked Objects for Direction ---
        for obj_id, obj_data in tracked_objects.items():
            if 'centroid' not in obj_data:
                continue # Skip if centroid is missing (e.g., from old, unmatched objects)

            x1, y1, x2, y2 = obj_data['bbox']
            centroid = obj_data['centroid']
            cls = obj_data['cls']

            # Determine direction and wrong way status
            direction_status = determine_direction(obj_id, centroid, frame_width, frame_height)
            if direction_status:
                object_direction_status[obj_id] = direction_status

            # Draw bounding box and centroid
            color = (0, 255, 0) # Green for correct way
            label = f"ID:{obj_id} {class_names[cls]}"

            # Check if this object is determined to be "wrong way"
            if object_direction_status.get(obj_id) == 'wrong':
                color = (0, 0, 255) # Red for wrong way
                label += " WRONG WAY!"

            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
            cv2.circle(frame, centroid, 5, color, -1) # Draw centroid

            # Put label
            cv2.putText(frame, label, (int(x1), int(y1) - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            # Draw centroid history (optional, for visualization)
            if obj_id in object_centroid_history:
                for i in range(1, len(object_centroid_history[obj_id])):
                    pt1 = object_centroid_history[obj_id][i-1]
                    pt2 = object_centroid_history[obj_id][i]
                    cv2.line(frame, pt1, pt2, (255, 255, 0), 1) # Yellow line for path

        # Draw the detection line
        if FLOW_TYPE == 'vertical_flow' and DETECTION_LINE_Y is not None:
            cv2.line(frame, (0, DETECTION_LINE_Y), (frame_width, DETECTION_LINE_Y), (255, 0, 0), 2) # Blue line
            cv2.putText(frame, "DETECTION LINE", (10, DETECTION_LINE_Y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            cv2.putText(frame, f"Expected: {EXPECTED_DIRECTION}", (10, DETECTION_LINE_Y + 25),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

        elif FLOW_TYPE == 'horizontal_flow' and DETECTION_LINE_X is not None:
            cv2.line(frame, (DETECTION_LINE_X, 0), (DETECTION_LINE_X, frame_height), (255, 0, 0), 2) # Blue line
            cv2.putText(frame, "DETECTION LINE", (DETECTION_LINE_X + 10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            cv2.putText(frame, f"Expected: {EXPECTED_DIRECTION}", (DETECTION_LINE_X + 10, 55),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)


        # Write the processed frame to the output video
        out.write(frame)

    # Release resources
    cap.release()
    out.release()
    cv2.destroyAllWindows()
    print(f"\nVideo processing complete. Output saved to {output_path}")

# ----------------------------------------------------------------------------
# 5. Example Usage
# ----------------------------------------------------------------------------

# --- Important: Upload your video file to Google Colab's file system first ---
# On the left sidebar, click the folder icon, then the upload icon.
# Upload a video file (e.g., 'traffic_video.mp4').

# Example usage: Replace 'your_video.mp4' with the path to your uploaded video.
# You can also use a public URL to a video if available (ensure it's a direct link).
# For example, if you upload a video named 'traffic_sample.mp4':
# video_file_path = '/content/traffic_sample.mp4'

# --- Placeholder: Download a sample video if you don't have one ---
# This uses a dummy video URL for demonstration. Replace with your actual video path.
# You can also record a short video of traffic (e.g., from a phone) and upload it.
sample_video_url = "https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4"
sample_video_name = "sample_traffic_video.mp4"

print(f"\nDownloading a sample video from {sample_video_url}...")
!wget -O {sample_video_name} {sample_video_url} --quiet
print(f"Sample video downloaded as {sample_video_name}")

video_file_path = f'/content/{sample_video_name}'
output_video_path = '/content/wrong_way_detected_output.mp4'

# Run the detection process
# IMPORTANT: Adjust FLOW_TYPE and EXPECTED_DIRECTION based on your video's content
# For the sample video, traffic appears to flow horizontally, so let's set it.
# Assuming traffic flows from LEFT to RIGHT in the sample video:
FLOW_TYPE = 'horizontal_flow'
EXPECTED_DIRECTION = 'right'
print(f"\nConfigured for {FLOW_TYPE} with expected direction: {EXPECTED_DIRECTION}")

process_video_for_wrong_way(video_file_path, output_video_path)

# ----------------------------------------------------------------------------
# 6. Post-processing and Download (Optional)
# ----------------------------------------------------------------------------

# Display the processed video in Colab (if it's a short video)
# Note: For very long videos, displaying directly in Colab might be slow or crash.
# You can download the video and watch it locally.
print("\nIf the video is short, it might be displayed below. Otherwise, download it.")
from IPython.display import HTML
from base64 import b64encode

try:
    mp4 = open(output_video_path, 'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
    HTML(f"""
    <video width="600" height="400" controls>
        <source src="{data_url}" type="video/mp4">
        Your browser does not support the video tag.
    </video>
    """)
except FileNotFoundError:
    print(f"Output video '{output_video_path}' not found. Check if processing completed successfully.")
except Exception as e:
    print(f"Could not display video in Colab: {e}")

print(f"\nTo download the output video, run the following in a new cell or use the Colab file browser:")
print(f"from google.colab import files")
print(f"files.download('{output_video_path}')")

"""
## In-depth Logic Explanation and Optimizations:

**1. YOLO Object Detection:**
-   **Model Choice:** We use `YOLOv8n` (nano version) from `ultralytics`. It's a good balance of speed and accuracy for general use cases on Colab's GPU. For higher accuracy, you can switch to `yolov8s`, `yolov8m`, `yolov8l`, or `yolov8x` by changing the model name (e.g., `model = YOLO('yolov8s.pt')`). The choice depends on your trade-off between performance and resource availability.
-   **Confidence Threshold (`CONFIDENCE_THRESHOLD`):** This parameter filters out weak detections. A higher value (e.g., 0.7) means fewer false positives but potentially more false negatives (missing actual vehicles). A lower value (e.g., 0.3) will detect more objects, including noisy ones. Tune this based on your video quality and desired sensitivity.
-   **IOU Threshold (`IOU_THRESHOLD`):** Used in Non-Maximum Suppression (NMS). It determines how much overlap is allowed between bounding boxes for the same object. A higher IOU threshold means more overlapping boxes might be kept, while a lower one aggressively merges them.
-   **Target Classes:** We explicitly filter for `car`, `motorcycle`, `bus`, `truck` (COCO dataset classes). This reduces irrelevant detections and focuses processing on vehicles.

**2. Centroid Calculation:**
-   The centroid (center point) of each bounding box `(cx, cy)` is calculated. This single point is used for tracking instead of the entire bounding box, simplifying movement calculations.

**3. Object Tracking (Basic IOU-based):**
-   **Purpose:** To assign a consistent `object_id` to the same vehicle across multiple frames. Without tracking, each vehicle would be treated as a new detection in every frame, making direction determination impossible.
-   **Method:** A simple Intersection Over Union (IOU) based approach is used. For each new detection in the current frame, it attempts to find the existing `tracked_object` with the highest IOU. If the IOU is above a certain threshold (e.g., 0.3), it's considered a match, and the existing `object_id` is retained.
-   **Limitations:** This basic tracker is prone to errors in crowded scenes, occlusions, or sudden movements. For robust real-world applications, consider integrating dedicated tracking algorithms like:
    -   **DeepSORT:** Combines detection with a Kalman filter and appearance features for more reliable long-term tracking.
    -   **ByteTrack:** A newer, efficient tracker that handles low-score detections effectively.
    -   **SORT (Simple Online and Realtime Tracking):** A simpler Kalman filter-based tracker.

**4. Direction Determination Logic (`determine_direction` function):**
-   **Centroid History (`object_centroid_history`):** A `deque` (double-ended queue) is used to store a fixed number of past centroids for each `object_id`. This history helps in smoothing out noisy movements and accurately calculating the general direction. `HISTORY_BUFFER_SIZE` controls how many past frames are considered.
-   **Movement Threshold (`MIN_MOVEMENT_PIXELS`):** Prevents spurious direction changes due to minor jitters in detection. A vehicle must move a minimum number of pixels in a consistent direction to trigger a direction classification.
-   **Flow Type and Expected Direction:**
    -   **`FLOW_TYPE`:** Crucial for defining the scene's traffic pattern. `vertical_flow` for roads where vehicles move up/down, `horizontal_flow` for left/right.
    -   **`EXPECTED_DIRECTION`:** Defines what constitutes "correct" movement within that flow type (e.g., 'down' for vertical, 'right' for horizontal).
-   **Detection Line (`DETECTION_LINE_Y` or `DETECTION_LINE_X`):** This is the **most critical component** for wrong-way detection. It's a virtual line drawn across the frame. The logic determines "wrong way" if a vehicle crosses this line *against* the `EXPECTED_DIRECTION` from a specific "wrong" zone.
    -   **Adaptation:** You MUST adjust the `DETECTION_LINE_Y` or `DETECTION_LINE_X` and `FLOW_TYPE`/`EXPECTED_DIRECTION` based on your specific camera setup and the road layout in your video. For example, if traffic is expected to go from right to left, `FLOW_TYPE` would be `'horizontal_flow'` and `EXPECTED_DIRECTION` would be `'left'`. The `DETECTION_LINE_X` would be around the middle of the road.

**5. Optimization Strategies:**

-   **Model Quantization/Pruning:** For deployment on edge devices or even faster inference on CPU, consider quantizing (e.g., to INT8) or pruning the YOLO model weights. `ultralytics` library might offer tools for this.
-   **Batch Processing:** If you have high-end GPUs, processing multiple frames in a batch can be faster than processing one by one, though `ultralytics` handles some internal optimizations.
-   **Region of Interest (ROI):** If wrong-way events only occur in a specific part of the frame, you can crop the frame to an ROI before feeding it to YOLO. This reduces computation.
-   **Multi-threading/Multi-processing:** For real-time applications, use separate threads or processes for video capture, detection, and rendering to parallelize tasks.
-   **GPU Usage:** Ensure your Colab runtime is set to GPU. `model.to('cuda')` ensures the model runs on GPU. The `ultralytics` library handles this automatically when GPU is available.
-   **Reduced Frame Rate:** For monitoring applications, you might not need to process every single frame. Processing every 2nd or 3rd frame can significantly reduce computational load while still capturing events.
-   **Kalman Filters for Tracking:** Integrating a Kalman filter for each `object_id` will provide smoother centroid predictions, even during brief occlusions, leading to more stable direction calculations.

**How to Use and Adapt:**

1.  **Upload Video:** Upload your traffic video (e.g., `my_traffic_video.mp4`) to the Colab environment using the file browser on the left.
2.  **Adjust `video_file_path`:** Change `video_file_path` in the "Example Usage" section to point to your uploaded video.
3.  **Crucially, Adjust `FLOW_TYPE` and `EXPECTED_DIRECTION`:**
    -   Watch your video to understand the primary direction of traffic flow.
    -   If traffic moves generally from top to bottom, set `FLOW_TYPE = 'vertical_flow'` and `EXPECTED_DIRECTION = 'down'`.
    -   If traffic moves generally from left to right, set `FLOW_TYPE = 'horizontal_flow'` and `EXPECTED_DIRECTION = 'right'`.
    -   Adjust the corresponding `DETECTION_LINE_Y` or `DETECTION_LINE_X` in the `determine_direction` function (or dynamically in `process_video_for_wrong_way`) if the default middle line isn't suitable.
4.  **Run All Cells:** Execute all cells in the notebook (`Runtime -> Run all`).
5.  **Review Output:** The processed video will be saved as `wrong_way_detected_output.mp4` in `/content/`. You can download it to review the results.

This code provides a strong foundation. Real-world scenarios often require more sophisticated tracking and fine-tuning of parameters.
"""
